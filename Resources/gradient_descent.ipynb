{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07ae748",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b90499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    List,\n",
    "    Dict,\n",
    "    Tuple\n",
    ")\n",
    "from numpy import (\n",
    "    array,\n",
    "    cov,\n",
    "    diag,\n",
    "    eye,\n",
    "    hstack,\n",
    "    ravel,\n",
    "    kron,\n",
    "    mean,\n",
    "    multiply,\n",
    "    ones,\n",
    "    savez_compressed,\n",
    "    sqrt,\n",
    "    squeeze,\n",
    "    vstack,\n",
    "    zeros,\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06cd6ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch GD final weights (bias, w): [2.08998074 3.10086975]\n",
      "Final MSE: 0.9605306224045634\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def make_linear_data(\n",
    "    n: int = 200, \n",
    "    weight: float = 3.0, \n",
    "    bias: float = 2.0, \n",
    "    noise: float = 1.0\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    x = np.random.randn(n, 1)\n",
    "    y = weight * x + bias + noise * np.random.randn(n, 1)\n",
    "    return x, y\n",
    "\n",
    "X, y = make_linear_data()\n",
    "X_b = hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "def mse_loss(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray\n",
    ") -> float:\n",
    " \n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def predict(\n",
    "    X: np.ndarray, \n",
    "    w: np.ndarray\n",
    ") -> np.ndarray:\n",
    "\n",
    "    return X.dot(w)\n",
    "\n",
    "def batch_gradient_descent(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.1, \n",
    "    epochs: int = 100\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    losses: List[float] = []\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = predict(X, w)\n",
    "        error = y_pred - y\n",
    "        grad = (2.0 / m) * X.T.dot(error)\n",
    "        w = w - learning_rate * grad\n",
    "        losses.append(mse_loss(y, y_pred))\n",
    "    return w, losses\n",
    "\n",
    "w_bgd, losses_bgd = batch_gradient_descent(X_b, y, learning_rate=0.1, epochs=200)\n",
    "print('Batch GD final weights (bias, w):', w_bgd.ravel())\n",
    "print('Final MSE:', losses_bgd[-1])\n",
    "\n",
    "DATA = dict(X=X, y=y, X_b=X_b)\n",
    "BGD_RESULT = dict(w=w_bgd, losses=losses_bgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5abba1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch final weights (bias, w): tensor(2.0900) tensor(3.1009)\n",
      "PyTorch final MSE: 0.960530698299408\n"
     ]
    }
   ],
   "source": [
    "X_t = torch.from_numpy(DATA['X']).float()\n",
    "y_t = torch.from_numpy(DATA['y']).float()\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(X_t)\n",
    "    loss = criterion(preds, y_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "w_t = model.weight.detach().ravel()[0]\n",
    "b_t = model.bias.detach().ravel()[0]\n",
    "print('PyTorch final weights (bias, w):', b_t, w_t)\n",
    "print('PyTorch final MSE:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e10a6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow final weights (bias, w): 2.0899804 3.1008692\n",
      "TensorFlow final MSE: 0.960530698299408\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Input(shape=(1,)), tf.keras.layers.Dense(1)])\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1), loss='mse')\n",
    "model.fit(DATA['X'], DATA['y'], epochs=200, batch_size=DATA['X'].shape[0], verbose=0)\n",
    "w_tf = model.layers[0].kernel.numpy().ravel()[0]\n",
    "b_tf = model.layers[0].bias.numpy().ravel()[0]\n",
    "loss_tf = model.evaluate(DATA['X'], DATA['y'], verbose=0)\n",
    "print('TensorFlow final weights (bias, w):', b_tf, w_tf)\n",
    "print('TensorFlow final MSE:', loss_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cde4575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD final weights (bias, w): [2.16366577 3.08969605]\n",
      "SGD final MSE: 0.9661351264515031\n"
     ]
    }
   ],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.01, \n",
    "    epochs: int = 5\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    losses: List[float] = []\n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(m)\n",
    "        for i in perm:\n",
    "            xi = X[i:i+1]\n",
    "            yi = y[i:i+1]\n",
    "            pred = xi.dot(w)\n",
    "            grad = 2.0 * xi.T.dot(pred - yi)\n",
    "            w = w - learning_rate * grad\n",
    "        losses.append(mse_loss(y, predict(X, w)))\n",
    "    return w, losses\n",
    "\n",
    "w_sgd, losses_sgd = stochastic_gradient_descent(DATA['X_b'], DATA['y'], learning_rate=0.01, epochs=50)\n",
    "print('SGD final weights (bias, w):', w_sgd.ravel())\n",
    "print('SGD final MSE:', losses_sgd[-1])\n",
    "SGD_RESULT = dict(w=w_sgd, losses=losses_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "256bf2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch SGD final weights (bias, w): 2.1767284870147705 3.095686674118042\n"
     ]
    }
   ],
   "source": [
    "X_t = torch.from_numpy(DATA['X']).float()\n",
    "y_t = torch.from_numpy(DATA['y']).float()\n",
    "dataset = TensorDataset(X_t, y_t)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(50):\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print('PyTorch SGD final weights (bias, w):', model.bias.item(), model.weight.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4046d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\miniconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow SGD final weights (bias, w): 2.093462 3.1641674\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((DATA['X'], DATA['y'])).shuffle(200).batch(1)\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "for epoch in range(50):\n",
    "    for xb, yb in ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(xb, training=True)\n",
    "            loss = loss_fn(yb, preds)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "w_tf = model.layers[0].kernel.numpy().ravel()[0]\n",
    "b_tf = model.layers[0].bias.numpy().ravel()[0]\n",
    "print('TensorFlow SGD final weights (bias, w):', b_tf, w_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch GD final weights (bias, w): [2.04203987 3.11233255]\n",
      "Mini-batch final MSE: 0.9629872982499463\n"
     ]
    }
   ],
   "source": [
    "def minibatch_gradient_descent(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.05, \n",
    "    epochs: int = 100, \n",
    "    batch_size: int = 20\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(m)\n",
    "        for i in range(0, m, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            xb = X[idx]\n",
    "            yb = y[idx]\n",
    "            pred = xb.dot(w)\n",
    "            grad = (2.0 / xb.shape[0]) * xb.T.dot(pred - yb)\n",
    "            w = w - learning_rate * grad\n",
    "        losses.append(mse_loss(y, predict(X, w)))\n",
    "    return w, losses\n",
    "\n",
    "w_mbgd, losses_mbgd = minibatch_gradient_descent(DATA['X_b'], DATA['y'], learning_rate=0.05, epochs=200, batch_size=32)\n",
    "print('Mini-batch GD final weights (bias, w):', w_mbgd.ravel())\n",
    "print('Mini-batch final MSE:', losses_mbgd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc2e0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch mini-batch final weights (bias, w): 2.0999948978424072 3.0703885555267334\n"
     ]
    }
   ],
   "source": [
    "X_t = torch.from_numpy(DATA['X']).float()\n",
    "y_t = torch.from_numpy(DATA['y']).float()\n",
    "dataset = TensorDataset(X_t, y_t)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(200):\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print('PyTorch mini-batch final weights (bias, w):', model.bias.item(), model.weight.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07353d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\miniconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow mini-batch final weights (bias, w): 2.058479 3.0675073\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.05), loss='mse')\n",
    "model.fit(DATA['X'], DATA['y'], epochs=200, batch_size=32, verbose=0)\n",
    "print('TensorFlow mini-batch final weights (bias, w):', model.layers[0].bias.numpy().ravel()[0], model.layers[0].kernel.numpy().ravel()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum-Based Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf354e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum GD final weights (bias, w): [2.09001287 3.10094807]\n",
      "Momentum final MSE: 0.960530628531266\n"
     ]
    }
   ],
   "source": [
    "def momentum_gradient_descent(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.05, \n",
    "    epochs: int = 100, \n",
    "    beta: float = 0.9\n",
    ")-> Tuple[np.ndarray, List[float]]:\n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    v = np.zeros_like(w)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = X.dot(w)\n",
    "        grad = (2.0 / m) * X.T.dot(y_pred - y)\n",
    "        v = beta * v + learning_rate * grad\n",
    "        w = w - v\n",
    "        losses.append(mse_loss(y, predict(X, w)))\n",
    "    return w, losses\n",
    "\n",
    "w_mom, losses_mom = momentum_gradient_descent(DATA['X_b'], DATA['y'], learning_rate=0.05, epochs=200)\n",
    "print('Momentum GD final weights (bias, w):', w_mom.ravel())\n",
    "print('Momentum final MSE:', losses_mom[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2999a8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Momentum final weights (bias, w): 2.0900185108184814 3.10093092918396\n"
     ]
    }
   ],
   "source": [
    "X_t = torch.from_numpy(DATA['X']).float()\n",
    "y_t = torch.from_numpy(DATA['y']).float()\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X_t)\n",
    "    loss = criterion(pred, y_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print('PyTorch Momentum final weights (bias, w):', model.bias.item(), model.weight.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff4d16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\miniconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Momentum final weights (bias, w): 2.09001 3.1009314\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9), loss='mse')\n",
    "model.fit(DATA['X'], DATA['y'], epochs=200, batch_size=DATA['X'].shape[0], verbose=0)\n",
    "print('TensorFlow Momentum final weights (bias, w):', model.layers[0].bias.numpy().ravel()[0], model.layers[0].kernel.numpy().ravel()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75815cfc",
   "metadata": {},
   "source": [
    "### Adaptive Gradient Descent (AdaGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c481b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaGrad final weights (bias, w): [2.08998071 3.10086935]\n",
      "AdaGrad final MSE: 0.9605306224047068\n"
     ]
    }
   ],
   "source": [
    "def adagrad(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.5, \n",
    "    epochs: int = 100, \n",
    "    eps: float = 1e-8\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    \n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    g_acc = np.zeros_like(w)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = X.dot(w)\n",
    "        grad = (2.0 / m) * X.T.dot(y_pred - y)\n",
    "        g_acc += grad * grad\n",
    "        adjusted_learning_rate = learning_rate / (np.sqrt(g_acc) + eps)\n",
    "        w = w - adjusted_learning_rate * grad\n",
    "        losses.append(mse_loss(y, predict(X, w)))\n",
    "    return w, losses\n",
    "\n",
    "w_adagrad, losses_adagrad = adagrad(DATA['X_b'], DATA['y'], learning_rate=0.5, epochs=200)\n",
    "print('AdaGrad final weights (bias, w):', w_adagrad.ravel())\n",
    "print('AdaGrad final MSE:', losses_adagrad[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75b349df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch AdaGrad final weights (bias, w): 2.089967966079712 3.100628614425659\n"
     ]
    }
   ],
   "source": [
    "X_t = torch.from_numpy(DATA['X']).float()\n",
    "y_t = torch.from_numpy(DATA['y']).float()\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X_t)\n",
    "    loss = criterion(pred, y_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print('PyTorch AdaGrad final weights (bias, w):', model.bias.item(), model.weight.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5ec28be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\miniconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow AdaGrad final weights (bias, w): 2.0899801 3.1008692\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.5), loss='mse')\n",
    "model.fit(DATA['X'], DATA['y'], epochs=200, batch_size=DATA['X'].shape[0], verbose=0)\n",
    "print('TensorFlow AdaGrad final weights (bias, w):', model.layers[0].bias.numpy().ravel()[0], model.layers[0].kernel.numpy().ravel()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b21beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSProp final weights (bias, w): [1.88754513 2.00681497]\n",
      "RMSProp final MSE: 2.0177368866177257\n"
     ]
    }
   ],
   "source": [
    "def rmsprop(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    lr: float = 0.01, \n",
    "    epochs: int = 200, \n",
    "    beta: float = 0.9, \n",
    "    eps: float = 1e-8\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    \n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    s = np.zeros_like(w)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = X.dot(w)\n",
    "        grad = (2.0 / m) * X.T.dot(y_pred - y)\n",
    "        s = beta * s + (1 - beta) * (grad * grad)\n",
    "        w = w - (lr / (np.sqrt(s) + eps)) * grad\n",
    "        losses.append(mse_loss(y, predict(X, w)))\n",
    "    return w, losses\n",
    "\n",
    "w_rms, losses_rms = rmsprop(DATA['X_b'], DATA['y'], lr=0.01, epochs=200)\n",
    "print('RMSProp final weights (bias, w):', w_rms.ravel())\n",
    "print('RMSProp final MSE:', losses_rms[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99b017ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch RMSProp final weights (bias, w): 1.9795793294906616 2.9544224739074707\n"
     ]
    }
   ],
   "source": [
    "X_t = torch.from_numpy(DATA['X']).float()\n",
    "y_t = torch.from_numpy(DATA['y']).float()\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X_t)\n",
    "    loss = criterion(pred, y_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print('PyTorch RMSProp final weights (bias, w):', model.bias.item(), model.weight.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d37c8e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\miniconda3\\envs\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow RMSProp final weights (bias, w): 1.8863635 1.9090302\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01), loss='mse')\n",
    "model.fit(DATA['X'], DATA['y'], epochs=200, batch_size=DATA['X'].shape[0], verbose=0)\n",
    "print('TensorFlow RMSProp final weights (bias, w):', model.layers[0].bias.numpy().ravel()[0], model.layers[0].kernel.numpy().ravel()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f327e",
   "metadata": {},
   "source": [
    "### ADAM: Adaptive Moment Estimation\n",
    "\n",
    "- Adam is a variant of SGD that uses adaptive learning rates for each parameter. It combines the benefits of both AdaGrad and RMSProp.\n",
    "- Mathematically, Adam is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38026d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam final weights (bias, w): [2.08993389 3.10095987]\n",
      "Adam final MSE: 0.960530631961434\n"
     ]
    }
   ],
   "source": [
    "def adam(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.1, \n",
    "    epochs: int = 200, \n",
    "    beta1: float = 0.9, \n",
    "    beta2: float = 0.999, \n",
    "    epsilon: float = 1e-8\n",
    ")-> Tuple[np.ndarray, List[float]]:\n",
    "    \n",
    "    m, n = X.shape\n",
    "    w = np.zeros((n, 1))\n",
    "    m_t = np.zeros_like(w)\n",
    "    v_t = np.zeros_like(w)\n",
    "    losses = []\n",
    "    for t in range(1, epochs + 1):\n",
    "        y_pred = X.dot(w)\n",
    "        grad = (2.0 / m) * X.T.dot(y_pred - y)\n",
    "        m_t = beta1 * m_t + (1 - beta1) * grad\n",
    "        v_t = beta2 * v_t + (1 - beta2) * (grad * grad)\n",
    "        m_hat = m_t / (1 - beta1 ** t)\n",
    "        v_hat = v_t / (1 - beta2 ** t)\n",
    "        w = w - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        losses.append(mse_loss(y, predict(X, w)))\n",
    "    return w, losses\n",
    "\n",
    "w_adam, losses_adam = adam(DATA['X_b'], DATA['y'], learning_rate=0.1, epochs=200)\n",
    "print('Adam final weights (bias, w):', w_adam.ravel())\n",
    "print('Adam final MSE:', losses_adam[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13c2e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Adam final weights (bias, w): 2.08988094329834 3.100698471069336\n"
     ]
    }
   ],
   "source": [
    "X_t = torch.from_numpy(DATA['X']).float()\n",
    "y_t = torch.from_numpy(DATA['y']).float()\n",
    "model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X_t)\n",
    "    loss = criterion(pred, y_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print('PyTorch Adam final weights (bias, w):', model.bias.item(), model.weight.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0195f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Adam final weights (bias, w): 2.0899477 3.1011472\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss='mse')\n",
    "model.fit(DATA['X'], DATA['y'], epochs=200, batch_size=DATA['X'].shape[0], verbose=0)\n",
    "print('TensorFlow Adam final weights (bias, w):', model.layers[0].bias.numpy().ravel()[0], model.layers[0].kernel.numpy().ravel()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
